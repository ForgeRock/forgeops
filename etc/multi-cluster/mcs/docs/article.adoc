= Deploy DS on GKE Multi-cluster Services (MCS)

:description: Deploy DS on GKE Multi-cluster Services. Step-by-step solution to make fully meshed replication, +
needed for HA of DS, work on GKE multi-cluster level.
:library: Asciidoctor
ifdef::asciidoctor[]
:source-highlighter: coderay
endif::asciidoctor[]
:idprefix:
:stylesheet: asciidoc.css
//:backend: docbook45
//:backend: html5
//:doctype: book
//:sectids!:
//:plus: &#43;

These instructions show how to deploy a multi-cluster DS solution that spans two GKE clusters from different
regions using MCS.

This solution allows pods in one GKE cluster to discover pods in another GKE cluster simplifying configuration by automating creation of external DNS and firewall rules.

NOTE: We will use the standard stateful applications (DS-CTS & DS-IDREPO) to deploy in each of the 2 GKE clusters and
scale them out and back using native Cloud Console Kubernetes scaling approach.


[[introduction,Introduction]]
== Introduction

For DS replication to work properly, the following criteria must be met:

. [[introduction-topology, Introduction point 1]]All servers in the topology must be able to connect to each
other; their network must be routed.
. [[introduction-FQDN, Introduction point 2]]FQDNs must be unique and resolvable by all servers.
. [[introduction-server-id, Introduction point 3: unique server ID in topology]]The server ID assigned to each server
in the topology must be
unique.
. [[introduction-bootstrap, Introduction point 4: bootstrap RS servers]]The DS replication bootstrap server settings
must include at least one server from each cluster in the topology.
. [[introduction-certificates, Introduction point 5]]The certificates used to establish server identities must be
verifiable,
by using
the
same CA or by properly
configuring the keystores.

The method described in this document explains how to put in place a configuration according to the requirements.

[[prerequisites,Prerequisites]]
== Prerequisites

* [[prerequisites-mcs-cluster-requirements, Prerequisites point 1]]2 GKE clusters running version 1.18.12+ with the following configuration: 
** Provisioned in the same VPC network
** VPC-native
** Workload Identity enabled

NOTE: this was the version used; the configuration might work on 1.17 or earlier)

* [[prerequisites-same-namespace, Prerequisites point 2]]Same namespace name on both GKE clusters.

NOTE: This restriction is imposed by the +secret-agent+ solution used to retrieve DS certificates.
For an alternative DS certificates storage/reconciliation solution, this restriction may not apply.

* [[prerequisites-nodes, Prerequisites point 3]]2+ nodes in each GKE cluster for tests to scale out/scale back

NOTE: tested configuration: the node pool with 2 machines of +e2-standard-8+ type (8 vCPU, 32 GB memory)

* Skaffold v1.19.0+
* Google Cloud SDK v331.0.0
* APIs required for MCS

```
gcloud services enable gkehub.googleapis.com --project <my-project-id>
gcloud services enable dns.googleapis.com --project <my-project-id>
gcloud services enable trafficdirector.googleapis.com --project <my-project-id>
gcloud services enable cloudresourcemanager.googleapis.com --project <my-project-id>
```


[[enable-MCS,Enabling MCS]]
== 1. Enabling MCS
To enable MCS, please complete the following steps:

[[create-ILB,Create internal load balancers in clusters]]
=== a. Enable the MCS API
```
gcloud services enable multiclusterservicediscovery.googleapis.com \
    --project <my-project-id>
```

[[enable-mcs,Enable MCS]]
=== b. Enable the MCS feature
```
gcloud alpha container hub multi-cluster-services enable \
    --project <my-project-id>
```

[[register-clusters,Register Clusters]]
=== c. Register your clusters to an environ
Please do not use any symbols in the membership name, just characters.  These names are also required as part of the fqdn when configuring server identifiers.
```
gcloud container hub memberships register <membershipname> \
   --gke-cluster <zone>/<cluster-name> \
   --enable-workload-identity
```
NOTE: Choose a membership name to uniquely identify the cluster

[[grant-permissions,Grant Permissions]]
=== d. Grant the required IAM permissions for MCS Importer
```
gcloud projects add-iam-policy-binding <my-project-id> \
    --member "serviceAccount:<my-project-id>.svc.id.goog[gke-mcs/gke-mcs-importer]" \
    --role "roles/compute.networkViewer"
```

[[verify-mcs,Verify MCS]]
=== e. Verify MCS is enabled
```
gcloud alpha container hub multi-cluster-services describe
```
look for `lifecycleState: ENABLED` in output

[[configure-sa,Configure secret agent]]
== 2. Configure secret agent

If your DS installation is not using the +secret-agent+ operator as a manager of certificates for server identity
verification  as mentioned in xref:introduction-certificates[], you can skip this step.

=== a. Configure access to Google Cloud Secret Manager

Follow instructions to configure secret-agent to work with Workload Identity: https://github.com/ForgeRock/secret-agent#set-up-cloud-backup-with-gcp-secret-manager[(Instructions)]  

This is required for both clusters to share the same secrets as required by DS.

=== b. Configure secret agent properties in SAC

The +multi-cluster-secrets/kustomization.yaml+ requires the following changes:

. +secretsManagerPrefix+ is changed to ensure uniqueness of stored secrets
. +secretsManager+ is changed to +GCP+ as a chosen Cloud Provider
. +gcpProjectID+ is changed in order to be able to use Secret Manager API

****
multi-cluster-secrets/kustomization.yaml https://github.com/ForgeRock/forgeops/tree/master/kustomize/overlay/multi-cluster/multi-cluster-secrets/kustomization.yaml[(latest version)]
```yaml
resources:
  - ../../../base/secrets

patchesStrategicMerge:
  - |-
    #Patch the SAC
    apiVersion: secret-agent.secrets.forgerock.io/v1alpha1
    kind: SecretAgentConfiguration
    metadata:
      name: forgerock-sac
    spec:
      appConfig:
        secretsManagerPrefix: "multi-cluster"
        secretsManager: GCP # none, AWS, Azure, or GCP
        gcpProjectID: engineering-devops
```
****

[[configure-service-export-object,Configure ServiceExport objects]]
== 3. Configure ServiceExport objects
MCS requires a Kubernetes service that can be exposed externally to other clusters for multi-cluster communication.  To expose the service, a ServiceExport object is required in each cluster.  The metadata.name of the ServiceExport object must match the name of the service.  For DS we expose the DS headless service.

****
us-export.yaml https://github.com/ForgeRock/forgeops/tree/master/etc/multi-cluster/mcs/files/us-export.yaml[(latest version)]
```yaml
kind: ServiceExport
apiVersion: net.gke.io/v1
metadata:
 namespace: prod
 name: ds-idrepo-us
---
kind: ServiceExport
apiVersion: net.gke.io/v1
metadata:
 namespace: prod
 name: ds-cts-us
```
****

The ServiceExport objects must be deployed first as they take approximately 5 minutes to sync to clusters registered in your environ. 

In US cluster:
```
kubectl create -f etc/multi-cluster/mcs/files/us-export.yaml 
```

In EU cluster:
```
kubectl create -f etc/multi-cluster/mcs/files/eu-export.yaml 
```

[[setup-DS,Setup DS configuration]]
== 4. Setup DS
Both DS-CTS and DS-IDREPO will be deployed on 2 clusters to simulate the ForgeRock stack.

This uses a ForgeOps configuration based on:

* Kustomize - a standalone tool to customize Kubernetes objects through a `kustomization.yaml` file

* Skaffold - a command line tool that facilitates continuous development for Kubernetes applications, handles the
workflow for building, pushing and deploying your application.

The examples show how to configure DS to be deployed on the US cluster. Apply a similar configuration for
the other cluster.

=== a. Prepare Kustomize definitions

*Make DS server ID unique*

To make the server ID of each pod in our topology unique, the DS service name must contain a cluster specific suffix.  This is done by adding the cluster suffix in the `kustomization.yaml` in each of the region's Kustomize overlay folders e.g.

****
kustomization.yaml https://github.com/ForgeRock/forgeops/tree/master/kustomize/overlay/multi-cluster/mcs-us/kustomization.yaml[(latest version)]
```
patches:
     - target:
         kind: Service
         name: ds-cts
       patch: |-
         - op: replace
           path: /metadata/name
           value: ds-cts-us
```
****

*Configure cluster topology*

For DS to configure the correct DS server identifiers, the following env vars must be configured.  These settings will be used in the docker-entrypoint.sh to ensure the DS pods are unique across both clusters.

See `kustomize/overlay/multi-cluster/mcs-<region>/kustomization.yaml`   

****
```
              env: 
              - name: DS_CLUSTER_TOPOLOGY
                value: "eu,us"
              - name: MCS_ENABLED
                value: "true"
```
****

DS_CLUSTER_TOPOLOGY must match the names given to the cluster membership name registered to the hub in section 1c xref:register-clusters[].  This is because the membership name is used as part of the FQDN required to reference pods behind a headless service.

Using these values, DS can dynamically configure the DS_BOOTSTRAP_REPLICATION_SERVERS and the DS_ADVERTISED_LISTEN_ADDRESS vars which results in the following FQDN:

```
HOSTNAME.MEMBERSHIP_NAME.SERVICE_NAME.NAMESPACE.svc.clusterset.local
```
Where:  

* HOSTNAME = pod hostname. 
* MEMBERSHIP_NAME = cluster membership name as configured in step 1c: xref:register-clusters[].  
* SERVICE_NAME = DS service name. 

Example FQDN for ds-idrepo-0 in US cluster would look like:  
```
ds-idrepo-0.us.ds-idrepo-us.prod.svc.clusterset.local
```

=== c. Prepare Skaffold profiles
Add following profile to the `skaffold.yaml`. Repeat for EU switching `us` for `eu`

****
skaffold.yaml https://github.com/ForgeRock/forgeops/blob/master/skaffold.yaml[(latest version)]
```
# Multi-cluster DS : US profile
- name: mcs-us
  build:
    artifacts:
    - *DS-CTS
    - *DS-IDREPO
    tagPolicy:
      sha256: { }
  deploy:
    kustomize:
      path: ./kustomize/overlay/multi-cluster/mcs-us
```
****

=== d. Deploy Skaffold profiles

Once the configuration for all clusters is in place, you can start the topology. Below is an example of a Skaffold command to run the pre-configured profile.

Deploy to US:
```
skaffold run --profile mcs-us
```

And for EU:
```
skaffold run --profile mcs-eu
```
[[load-tests,Run load tests]]
=== 5. Load tests

=== a. Addrate load test

Some basic load was added on a deployment consisting of three replicated servers, one in Europe and two in the US clusters, just to make sure the setup did not have any major problems, independently of absolute numbers.
The `addrate` load was tested on the server in Europe
on CTS-like entries for 30mins.
A screenshot from Grafana shows the behaviour of the two servers in the US:

image::addrate-30mins.png[]

Both US servers are closely following the client load demonstrated by the low replication delay. There are some outliers but replication recovers easily.

Greater testing was carried out on the kube-dns solution and results were comparable.  Please see that documentation for more in depth test results on addrate and modrate.

[[pricing,Pricing]]
== 6. Pricing

The only additional costs are CloudDNS costs for the dynamically generated DNS records.

== 6. Pros and Cons

|===
|Pros |Cons

|1. Native Kubernetes solution: only modifies K8S objects
|1. Specific configuration of server identifiers handled in docker-entrypoint.sh. Requires the correct values to be set to work correctly.

|2. Simple installation: automatic generation of DNS records and firewall rules
|2. MCS managed Services generate healthchecks which are based on the service endpoint which requires a client secret.  This currently fails as the healthcheck is unconfigurable

|3. Scale out/scale back using Kubernetes: no additional administration
|3. We expose the whole DS service to each cluster even though we only need to expose port 8989.  This isn't configurable.

|4. No additional scripts required
|

|5. Supported by Google.
|

|6. So far, tests are reassuring: replication latency is acceptable
|
|===