# Copyright (c) 2016-2018 ForgeRock AS.


# If useDefaultSecrets is set to true (the default), the secret values in ../secrets will
# be used to create a secret map with the same name as the instance ($djInstance).
# If you set useDefaultSecrets to false, you must create this secret map yourself before the DS
# instances will be provisioned. This allows you to inject your own secrets rather
# than use the default ones bundled in the chart. An alternate strategy is to fork this chart.
# and replace the secrets in ./secrets with your own.
useDefaultSecrets: true

# The default OpenDJ baseDN.
# Note that the default install creates two backends: o=cts and o=userstore
# The setting below changes the base dn for the o=userstore backend only!
# We strongly suggest that you *NOT* change the default. There is very little value in
# having the basedn reflect a unique organizational structure.
baseDN: "o=userstore"

# The default instance name. This will create a stateful set that can be resolved at
#  $djInstance-0.$djInstance
djInstance: ds

component: ds

image:
  repository: gcr.io/engineering-devops
  # For development we set this to Always to get the most current image.
  pullPolicy: Always
  #pullPolicy: IfNotPresent
  tag:  6.5.0

# The number of instances in the StatefulSet. Each instance will be replicated to the master.
replicas: 1

# Set for creating sample users. If unset, only the base DN will be created.
#numberSampleUsers: 500

# Size for OpenDJ database storage. Note GKE IOPS scale based on the size of the volume.
storageSize: "10Gi"

# StorageClass is optional. It can be the name of a fast (SSD) class.
#storageClass: fast

backup:
  # If true this will mount a PVC of the given storage class on /opt/opendj/bak/
  enabled: false
  storageClass: "nfs"
  storageSize: "20Gi"
  # If you provide the name of a backup PVC, the default bak-$instance will NOT be created, and
  # the named pvc will be used as the shared backup volume. This can be used to re-attach 
  # a previous backup pvc - assuming it was not deleted.
  # for example, to reuse the previous volume  pvcName: userstore-bak
  #pvcName: 
  # Backup schedule. The first backup is a full, subsequent backups are incremental
  # Format is crontab: minutes hour day-of-month month day-of-week
  schedule:  "*/30 * * * *"
  # If true, run a verification job
  verify: false


# Set storageClass only on clusters that support it (GCP / AWS).
#storageClass: fast

# You need to be on JDK 8u131 or higher to enable these options.
# todo: find JDK 11 args
#opendjJavaArgs: "-server -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:+UseCompressedOops -XX:+UseG1GC -XX:MaxGCPauseMillis=100 -XX:MaxRAMFraction=2"

opendjJavaArgs: "-Xmx1024m -XX:+UnlockDiagnosticVMOptions"
# Resource limits.
# These help for pod placement on a larger cluster to ensure the OpenDJ instance gets sufficient resources.
# For production you will want to specify CPU limits to help Kube schedule the pods.
resources: {}

# Sample values:
#resources:
#  requests:
#   memory: 2000Mi
#  limits:
#    memory: 2500Mi

# DJ persistence switch. Setting this to false disables volume claims - all data is stored inside the docker image.
# Used in testing environments without pv providers. When the pod is terminated, the DJ data will be deleted!
# If you run into issues with minikube set this to false. See https://github.com/kubernetes/minikube/issues/1990
djPersistence: true

# Pod Anti Affinity switch. For production this should be set to "hard", otherwise use "soft".
# The hard setting means that do not deploy more than one pod in a node which is critical for HA.
podAntiAffinity: "soft"

# This is the exact value for TopologyKey. The other possible value is "failure-domain.beta.kubernetes.io/zone"
# which will ensure that pod is scheduled on nodes in different zones thus allowing for HA across zones.
# Note you want to leave this value as is if you are deploying a single zone cluster and change the values only
# if you have a multi-zone cluster.
topologyKey: "kubernetes.io/hostname"

gcs:
  # Set this to true to enable backups to Google Cloud Storage. You need to create the top level bucket first.
  backup: false
  # Set to true to enable restoring data from a gcs bucket to the bak/ folder.
  # Note: This does not trigger a DS restore. It only gets the data from GCS to the bak/ folder
  # See the restore.enabled setting.
  restore: false
  # backup Bucket destination. You need the right scopes in container engine to write to this bucket.
  #add --scopes storage-full when you create the cluster.
  # Note the namespace will be appended to this bucket path. This is done to avoid collisions where 
  # multiple backups may be ocurring in different namespaces
  backupBucket:  gs://forgeops/dj-backup/test
  # Restore bucket. Usually the same as above, but for initialization we might want to restore from another source.
  # Note the namespace is *not* appended here. This is so you can restore from a fixed location, or
  # restore from say prod to qa.
  restoreBucket: gs://forgeops/dj-backup/test

# Restore parameters. 
restore:
  # If true, runs the init containers that restores the directory from a backup in the bak/ folder.
  # The backup data must be present in the bak/ folder. The gcs: settings can be used
  # to copy data from a gcs bucket to the bak/ folder.
  # Restore will not overwrite existing DS data.  
  # The most recent backup will be restored. 
  enabled: false
 
# DS proxy - experimental.
proxy:
  enabled: false

haproxy:
  enabled: false
  replicas: 1

# An optional slack webhook url. It can be used by the backup and verification processes to post notifications to slack.
# If you don't have slack, set this to "undefined"
slackUrl: undefined

# Uncomment if you want to run an additional "admin" ds pod that you can exec into and run DS commands.
# Useful for debugging.
#adminContainer: true

# Pinned gcloud Image. You should not need to change this very often
gcloudImage: gcr.io/cloud-builders/gcloud@sha256:92166f0671d049e203ab86dd7fed80058c75b1faed01ca2f8ea8b48a97ea081b